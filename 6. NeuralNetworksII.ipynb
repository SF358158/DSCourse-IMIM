{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t08IWzpL-6jF",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "Neural nets are a specific method for learning from data, a method that is based on a very simple element, the *neuron unit*. A neuron unit (or 1-layer neural network) is a mathematical function of this kind:\n",
        "\n",
        "${\\mathbf y} = \\sigma(\\mathbf{w}^T \\cdot {\\mathbf x} + b)$\n",
        "\n",
        "where ${\\mathbf x}$ represents an input element in vector form, $\\mathbf{w}$ is a vector of weights,  $\\sigma$ is a non-linear function and $b$ a scalar value. $(\\mathbf{w},b)$ are called the parameters of the function. The output of this function is called the *activation* of the neuron. \n",
        "\n",
        "Regarding the non-linear function, historically the most common one was the Sigmoid function, but nowadays there are several alternatives that are supposed to be better suited to learning from data, such as ReLU and variants.\n",
        "\n",
        "> **Q:** What kind of decision functions are represented by a 1-layer nn?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX5NwyDo_A_P",
        "colab_type": "code",
        "outputId": "acd96624-86c2-4f1c-bf71-1fcd66e9b67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "plt.ylim(-1.5, 10)\n",
        "x = np.linspace(-10.0,10.0,100)\n",
        "y1 = sigmoid(x)\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(x,y1)\n",
        "y2 = ReLU(x)\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(x,y2,'r')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5771c046d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX5LERmbBQaR",
        "colab_type": "code",
        "outputId": "1f8a1fc9-0322-48be-b0a0-d6398ab4bfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = np.array([0.4,1.2,3.5])\n",
        "\n",
        "w = np.array([1.0,2.0,1.0])\n",
        "b = 1.3\n",
        "\n",
        "y = sigmoid(np.dot(x,w) + b)\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9994997988929205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P7yHSro_Inc",
        "colab_type": "text"
      },
      "source": [
        "## Multilayer neural networks\n",
        "\n",
        "Simple neurons can be organized in larger structures by applying to the same data vector different sets of weights, forming what is called a *layer*, and by stacking layers one on top of the output of the other.  \n",
        "\n",
        "It is important to notice that a multilayer neural network can be seen as a composition of matrix products (matrices represent weights) and non-linear function activations. For the case of a 2-layer network the outcome is:\n",
        "\n",
        "$ {\\mathbf y} = {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big)$\n",
        "\n",
        "where ${\\mathbf \\sigma}$ represents a vectorial version of the sigmoid function and $W^i$ are the weights of each layer in matrix form.  \n",
        "\n",
        "What is interesting about this kind of structures is that it has been showed that even a neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function of $\\mathbf{R}^n$. This fact makes neural networks a sound candidate to implement learning from data methods. The question is then: how to find the optimal parameters, ${\\mathbf w} = (W^i,{\\mathbf b})$, to approximate a function that is implicitly defined by a set of samples $\\{({\\mathbf x}_1, {\\mathbf y}_1), \\dots,  ({\\mathbf x}_n, {\\mathbf y}_n)\\}$?\n",
        "\n",
        "From a technical point of view, not only neural networks but most of the algorithms that have been proposed to infer models from large data sets are based on the iterative solution of a mathematical problem that involves data and a mathematical model. If there was an analytic solution to the problem, this should be the adopted one, but this is not the case for most of the cases. The techniques that have been designed to tackle these problems are grouped under a field that is called optimization. The most important technique for solving optimization problems is *gradient descend*.\n",
        "\n",
        "> The training of models like $ {\\mathbf y} = {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big)$ (or bigger!) can be readily performed by applying *automatic differentiation* to a loss function. \n",
        "\n",
        "> In the case of regression: $L = \\frac{1}{n} \\sum_{i=1}^n \\Big({\\mathbf y}_i - {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x}_i + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 )\\Big)\\Big)^2 $\n",
        "\n",
        "> In the case of two-class classification: $L = \\frac{1}{n} log(1 + exp(-y_i {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big))) $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMoVJNwZB81F",
        "colab_type": "text"
      },
      "source": [
        "## Playing with neural nets.\n",
        "+ Concentric classes, 1 layer, Sigmoid.\n",
        "+ Concentric classes, 1 layer, ReLu.\n",
        "+ X-or, 0 layer.\n",
        "+ X-or, 1 layer.\n",
        "+ Spiral data.\n",
        "+ Regression.\n",
        "\n",
        "\n",
        "http://playground.tensorflow.org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqXdykt2DYo0",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning in `keras`\n",
        "\n",
        "> Keras is a high-level neural networks library, written in Python and capable of running on top TensorFlow. It was developed with a focus on enabling fast experimentation.\n",
        "\n",
        "The core data structure of Keras is a model, a way to organize layers. The main type of model is the ``Sequential model``, a linear stack of layers. \n",
        "\n",
        "```Python\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "```\n",
        "\n",
        "Stacking layers is as easy as ``.add()``:\n",
        "\n",
        "```Python\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "model.add(Dense(output_dim=64, input_dim=100))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(output_dim=10))\n",
        "model.add(Activation(\"softmax\"))\n",
        "```\n",
        "\n",
        "Once your model looks good, configure its learning process with ``.compile()``:\n",
        "\n",
        "```Python\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='sgd', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "If you need to, you can further configure your optimizer.\n",
        "\n",
        "```Python\n",
        "from keras.optimizers import SGD\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
        "```\n",
        "\n",
        "You can now iterate on your training data in batches:\n",
        "\n",
        "```Python\n",
        "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
        "```\n",
        "\n",
        "Evaluate your performance in one line:\n",
        "```Python\n",
        "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
        "```\n",
        "\n",
        "Or generate predictions on new data:\n",
        "\n",
        "```Python\n",
        "classes = model.predict_classes(X_test, batch_size=32)\n",
        "proba = model.predict_proba(X_test, batch_size=32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqHskK3EZQx_",
        "colab_type": "code",
        "outputId": "1b785a46-512e-480e-a300-982d94006a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3htJMJEualOT",
        "colab_type": "code",
        "outputId": "34b502a9-9bc1-414f-a176-e1f0e5c222a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4033626981049709055, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 6260323641622972063\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5171876941613213224\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11330115994\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 12792273479971568433\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEufgoGHDytG",
        "colab_type": "code",
        "outputId": "670359b3-8fbc-4eb1-ae9f-b14552eb0af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "'''Trains a simple deep NN on the MNIST dataset.\n",
        "Gets to 98.40% test accuracy after 20 epochs\n",
        "(there is *a lot* of margin for parameter tuning).\n",
        "2 seconds per epoch on a K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1113 15:24:32.411875 140014066894720 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2482 - acc: 0.9241 - val_loss: 0.1088 - val_acc: 0.9672\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1011 - acc: 0.9685 - val_loss: 0.0985 - val_acc: 0.9684\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0745 - acc: 0.9778 - val_loss: 0.0803 - val_acc: 0.9770\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0603 - acc: 0.9817 - val_loss: 0.0680 - val_acc: 0.9809\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0504 - acc: 0.9852 - val_loss: 0.0778 - val_acc: 0.9792\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0441 - acc: 0.9866 - val_loss: 0.0738 - val_acc: 0.9826\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0399 - acc: 0.9883 - val_loss: 0.0802 - val_acc: 0.9811\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0338 - acc: 0.9900 - val_loss: 0.0748 - val_acc: 0.9826\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0304 - acc: 0.9913 - val_loss: 0.0841 - val_acc: 0.9825\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0276 - acc: 0.9919 - val_loss: 0.0907 - val_acc: 0.9828\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0275 - acc: 0.9921 - val_loss: 0.1033 - val_acc: 0.9819\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0257 - acc: 0.9932 - val_loss: 0.1077 - val_acc: 0.9802\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0240 - acc: 0.9933 - val_loss: 0.0946 - val_acc: 0.9836\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0217 - acc: 0.9941 - val_loss: 0.1042 - val_acc: 0.9823\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0236 - acc: 0.9937 - val_loss: 0.0896 - val_acc: 0.9847\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0197 - acc: 0.9942 - val_loss: 0.1102 - val_acc: 0.9842\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0187 - acc: 0.9950 - val_loss: 0.1166 - val_acc: 0.9834\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0195 - acc: 0.9946 - val_loss: 0.1177 - val_acc: 0.9837\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0197 - acc: 0.9948 - val_loss: 0.1077 - val_acc: 0.9839\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0183 - acc: 0.9954 - val_loss: 0.1090 - val_acc: 0.9853\n",
            "Test loss: 0.10901042845614775\n",
            "Test accuracy: 0.9853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljANZSaIYe8I",
        "colab_type": "text"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Dropout is a way to regularize the neural network. During training, it may happen that neurons of a particular layer may always become influenced only by the output of a particular neuron in the previous layer. In that case, the neural network would overfit.\n",
        "\n",
        "Dropout prevents overfitting and regularizes by randomly cutting the connections (also known as dropping the connection) between neurons in successuve layers during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i22znQv4bA1Q",
        "colab_type": "text"
      },
      "source": [
        "### Keras optimizers\n",
        "\n",
        "There are several variants of gradient descent, which differ in how we compute the step.\n",
        "\n",
        "Keras supports seven optimizers.\n",
        "\n",
        "```python\n",
        "my_opt = K.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "my_opt = K.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "my_opt = K.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "my_opt = K.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "my_opt = K.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "my_opt = K.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
        "my_opt = K.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "```\n",
        "\n",
        "#### Momentum\n",
        "\n",
        "For example, SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.\n",
        "\n",
        "**Momentum** is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector:\n",
        "\n",
        "$$ v_t = m v_{t-1} + \\alpha \\nabla_w f $$$$ w = w - v_t    $$\n",
        "\n",
        "The momentum $m$ is commonly set to $0.9$.\n",
        "\n",
        "#### Adagrad\n",
        "\n",
        "SGD manipulates the learning rate globally and equally for all parameters. Tuning the learning rates is an expensive process, so much work has gone into devising methods that can adaptively tune the learning rates, and even do so per parameter.\n",
        "\n",
        "Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\n",
        "\n",
        "$$ c = c + (\\nabla_w f)^2 $$$$ w = w - \\frac{\\alpha}{\\sqrt{c}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3cSy4_H6aEi",
        "colab_type": "text"
      },
      "source": [
        "## CNN\n",
        "\n",
        "The previously mentioned multilayer perceptrons represent the most general and powerful feedforward neural network model possible; they are organised in layers, such that every neuron within a layer receives its own copy of all the outputs of the previous layer as its input. This kind of model is perfect for the right kind of problem – learning from a fixed number of (more or less) unstructured parameters.\n",
        "\n",
        "> However, consider what happens to the number of parameters (weights) of such a model when being fed raw image data (f.e. a $200 \\times 200$ pixel image connected to 1024 neurons)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxR3h2-d6nGn",
        "colab_type": "code",
        "outputId": "44d7ccc2-a157-4172-acfb-3ce54e826a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "200 * 200 * 1024"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40960000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlCN_f8E7OZ2",
        "colab_type": "text"
      },
      "source": [
        "The situation quickly becomes unmanageable as image sizes grow larger, way before reaching the kind of images people usually want to work with in real applications.\n",
        "\n",
        "A common solution is to downsample the images to a size where MLPs can safely be applied. However, if we directly downsample the image, we potentially lose a wealth of information; it would be great if we would somehow be able to still do some useful (without causing an explosion in parameter count) processing of the image, prior to performing the downsampling.\n",
        "\n",
        "It turns out that there is a very efficient way of pulling this off, and it makes advantage of the structure of the information encoded within an image – it is assumed that pixels that are spatially closer together will \"cooperate\" on forming a particular feature of interest much more than ones on opposite corners of the image. Also, if a particular (smaller) feature is found to be of great importance when defining an image's label, it will be equally important if this feature was found anywhere within the image, regardless of location.\n",
        "\n",
        "Enter the convolution operator. Given a two-dimensional image, $I$, and a small matrix, $K$ of size $h \\times w$, (known as a convolution kernel), which we assume encodes a way of extracting an interesting image feature, we compute the convolved image, $I∗K$, by overlaying the kernel on top of the image in all possible ways, and recording the sum of elementwise products between the image and the kernel:\n",
        "\n",
        "$$\n",
        "output(x,y) = (I \\otimes K)(x,y) = \\sum_{m=0}^{M-1} \\sum_{n=1}^{N-1} K(m,n) I(x-n, y-m)\n",
        "$$\n",
        "\n",
        "The convolution operator forms the fundamental basis of the convolutional layer of a CNN. The layer is completely specified by a certain number of kernels, $K$, and it operates by computing the convolution of the output images of a previous layer with each of those kernels, afterwards adding the biases (one per each output image). Finally, an activation function, $\\sigma$, may be applied to all of the pixels of the output images. \n",
        "\n",
        "Typically, the input to a convolutional layer will have $d$ channels (e.g., red/green/blue in the input layer), in which case the kernels are extended to have this number of channels as well.\n",
        "\n",
        "Note that, since all we're doing here is addition and scaling of the input pixels, the kernels may be learned from a given training dataset via gradient descent, exactly as the weights of an MLP. In fact, an MLP is perfectly capable of replicating a convolutional layer, but it would require a lot more training time (and data) to learn to approximate that mode of operation.\n",
        "\n",
        "## Pooling\n",
        "\n",
        "In fact, after a convolutional layer there are two kinds of non linear functions that are usually applied: non-linear activation functions such as sigmoids or ReLU and *pooling*. Pooling layers are used with the purpose to progressively reduce the spatial size of the image to achieve scale invariance. The most common layer is the *maxpool* layer. Basically a maxpool of $2 \\times 2$ causes a filter of 2 by 2 to traverse over the entire input array and pick the largest element from the window to be included in the next representation map. Pooling can also be implemented by using other criteria, such as averaging instead of taking the max element. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvbTpVNS7COv",
        "colab_type": "code",
        "outputId": "b58744af-f48d-4f59-a4b0-764e1929a8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10 # subroutines for fetching the CIFAR-10 dataset\n",
        "from tensorflow.keras.models import Model # basic class for specifying and training a neural network\n",
        "from tensorflow.keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "num_epochs = 200 # we iterate 200 times over the entire training set\n",
        "kernel_size = 3 # we will use 3x3 kernels throughout\n",
        "pool_size = 2 # we will use 2x2 pooling throughout\n",
        "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
        "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
        "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
        "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
        "hidden_size = 512 # the FC layer will have 512 neurons\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # fetch CIFAR-10 data\n",
        "\n",
        "num_train, height, width, depth = X_train.shape # there are 50000 training examples in CIFAR-10 \n",
        "num_test = X_test.shape[0] # there are 10000 test examples in CIFAR-10\n",
        "num_classes = np.unique(y_train).shape[0] # there are 10 image classes\n",
        "\n",
        "X_train = X_train.astype('float32') \n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= np.max(X_train) # Normalise data to [0, 1] range\n",
        "X_test /= np.max(X_test) # Normalise data to [0, 1] range\n",
        "\n",
        "Y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "Y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "inp = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
        "conv_1 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(inp)\n",
        "conv_2 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(conv_1)\n",
        "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
        "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
        "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
        "conv_3 = Convolution2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(drop_1)\n",
        "conv_4 = Convolution2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(conv_3)\n",
        "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
        "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
        "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
        "flat = Flatten()(drop_2)\n",
        "hidden = Dense(hidden_size, activation='relu')(flat)\n",
        "drop_3 = Dropout(drop_prob_2)(hidden)\n",
        "out = Dense(num_classes, activation='softmax')(drop_3)\n",
        "\n",
        "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "              optimizer='adam', # using the Adam optimiser\n",
        "              metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "model.fit(X_train, Y_train,                # Train the model using the training set...\n",
        "          batch_size=batch_size, epochs=num_epochs,\n",
        "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
        "model.evaluate(X_test, Y_test, verbose=1)  # Evaluate the trained model on the test set!"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/200\n",
            "45000/45000 [==============================] - 20s 441us/sample - loss: 1.4983 - acc: 0.4554 - val_loss: 1.1253 - val_acc: 0.6026\n",
            "Epoch 2/200\n",
            "45000/45000 [==============================] - 17s 368us/sample - loss: 1.1149 - acc: 0.6023 - val_loss: 0.9383 - val_acc: 0.6692\n",
            "Epoch 3/200\n",
            "45000/45000 [==============================] - 16s 364us/sample - loss: 0.9648 - acc: 0.6603 - val_loss: 0.8321 - val_acc: 0.7110\n",
            "Epoch 4/200\n",
            "45000/45000 [==============================] - 16s 364us/sample - loss: 0.8727 - acc: 0.6932 - val_loss: 0.8033 - val_acc: 0.7284\n",
            "Epoch 5/200\n",
            "45000/45000 [==============================] - 16s 365us/sample - loss: 0.8035 - acc: 0.7185 - val_loss: 0.7413 - val_acc: 0.7498\n",
            "Epoch 6/200\n",
            "45000/45000 [==============================] - 16s 363us/sample - loss: 0.7451 - acc: 0.7382 - val_loss: 0.7679 - val_acc: 0.7374\n",
            "Epoch 7/200\n",
            "45000/45000 [==============================] - 16s 358us/sample - loss: 0.6969 - acc: 0.7518 - val_loss: 0.7317 - val_acc: 0.7524\n",
            "Epoch 8/200\n",
            "45000/45000 [==============================] - 16s 360us/sample - loss: 0.6651 - acc: 0.7678 - val_loss: 0.6737 - val_acc: 0.7720\n",
            "Epoch 9/200\n",
            "45000/45000 [==============================] - 16s 362us/sample - loss: 0.6288 - acc: 0.7763 - val_loss: 0.6491 - val_acc: 0.7810\n",
            "Epoch 10/200\n",
            "32672/45000 [====================>.........] - ETA: 4s - loss: 0.5987 - acc: 0.7876"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-01140ece99fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m model.fit(X_train, Y_train,                # Train the model using the training set...\n\u001b[1;32m     55\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m           verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Evaluate the trained model on the test set!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8-AH0Zy96Aa",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "\n",
        "Classical neural networks, including convolutional ones, suffer from two severe limitations:\n",
        "\n",
        "+ They only accept a fixed-sized vector as input and produce a fixed-sized vector as output.\n",
        "+ They do not consider the sequential nature of some data (language, video frames, time series, etc.) \n",
        "\n",
        "Recurrent neural networks (RNN) overcome these limitations by allowing to operate over sequences of vectors (in the input, in the output, or both). RNNs are called recurrent because they perform the same task for every element of the sequence, with the output depending on the previous computations. The basic formulas of a simple RNN are:\n",
        "\n",
        "$$ s_t = f_1 (Ux_t + W s_{t-1}) $$\n",
        "$$ y_t = f_2 (V s_t) $$\n",
        "\n",
        "These equations basically say that the current network state, commonly known as hidden state, $s_t$ is a function $f_1$ of the previous hidden state $s_{t-1}$ and the current input $x_t$. $U, V, W$ matrices are the parameters of the function. \n",
        "\n",
        "Given an input sequence, we apply RNN formulas in a recurrent way until we process all input elements. The RNN shares the parameters  $U,V,W$ across all recurrent steps. We can think of the hidden state  as a memory of the network that captures information about the previous steps.\n",
        "\n",
        "The novelty of this type of network is that we we have encoded in the very architecture of the network a sequence modeling scheme that has been in used in the past to predict time series as well as to model language. In contrast to the precedent architectures we have introduced, now the hidden layers are indexed by both 'spatial' and 'temporal' index. \n",
        "\n",
        "The inputs of a recurrent network are always vectors, but we can process sequences of symbols/words by representing these symbols by numerical vectors.\n",
        "\n",
        "Let's suppose we want to classify a phrase or a series of words. Let $x^1, ...,x^{C}$ the word vectors corresponding to a corpus with $C$ symbols. Then, the relationship to compute the hidden layer output features at each time-step $t$ is $h_t = \\sigma(W s_{t-1} + U x_{t})$, where:\n",
        "\n",
        "+ $x_{t} \\in \\mathbf{R}^{d}$ is input word vector at time $t$. \n",
        "+ $U \\in \\mathbf{R}^{D_h \\times d}$ is the weights matrix of the input word vector, $x_t$.\n",
        "+ $W \\in \\mathbf{R}^{D_h \\times D_h}$ is the weights matrix of the output of the previous time-step, $t-1$.\n",
        "+ $s_{t-1}  \\in \\mathbf{R}^{D_h}$ is the output of the non-linear function at the previous time-step, $t-1$. \n",
        "+ $\\sigma ()$ is the non-linearity function (normally, ``tanh``).\n",
        "\n",
        "\n",
        "The output of this network is $\\hat{y}_t = softmax (V h_t)$, that represents the output probability distribution over the vocabulary at each time-step $t$.  \n",
        "\n",
        "Essentially, $\\hat{y}_t$ is the next predicted word given the document context score so far (i.e. $h_{t-1}$) and the last observed word vector $x^{(t)}$. \n",
        "\n",
        "The loss function used in RNNs is often the cross entropy error:\n",
        "\n",
        "$\n",
        "\tL^{(t)}(W) = - \\sum_{j=1}^{|V|} y_{t,j} \\times log (\\hat{y}_{t,j})\n",
        "$\n",
        "\n",
        "The cross entropy error over a corpus of size $C$ is:\n",
        "\n",
        "$\n",
        "\tL = \\frac{1}{C} \\sum_{c=1}^{C} L^{(c)}(W) = - \\frac{1}{C} \\sum_{c=1}^{C} \\sum_{j=1}^{|V|} y_{c,j} \\times log (\\hat{y}_{c,j})\n",
        "$\n",
        "\n",
        "These simple RNN architectures have been shown to be too prone to forget information when sequences are long and they are also very unstable when trained. For this reason several alternative architectures have been proposed. These alternatives are based on the presence of *gated units*. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The two most important alternative RNN are Long Short Term Memories (LSTM) and Gated Recurrent Units (GRU) networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayj2JDJ87zKu",
        "colab_type": "code",
        "outputId": "24461e27-0b85-42db-cfb5-1fdc4b890aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Example script to generate text from Nietzsche's writings.\n",
        "At least 20 epochs are required before the generated text\n",
        "starts sounding coherent.\n",
        "It is recommended to run this script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "If you try this script on new data, make sure your corpus\n",
        "has at least ~100k characters. ~1M is better.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=60,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 1s 1us/step\n",
            "614400/600901 [==============================] - 1s 1us/step\n",
            "corpus length: 600893\n",
            "total chars: 57\n",
            "nb sequences: 200285\n",
            "Vectorization...\n",
            "Build model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1113 15:37:38.660059 140014066894720 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 200285 samples\n",
            "Epoch 1/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.9756\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"d\n",
            "evinces courage, directness, endurance\"\n",
            "d\n",
            "evinces courage, directness, endurance of the fact the present and the prosent of the pearations and the prospect of the pear to the pearations of the possess of the possess of the possess the presenting and for the possical to a strength the present and the believe and the prostitions and the possess the possitions and the possical compleation of the present and farse of the pease of the possess and the perhaps the pease of the prese\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"d\n",
            "evinces courage, directness, endurance\"\n",
            "d\n",
            "evinces courage, directness, endurance the seemen as even the revence the peans the end man, and the man, been with standing the plant to complies and the purposent and precent of care of the pore of decending the present of the fact of be the plase and was a tre of a self-generations of the peaple of was of the every pains then even the more present and present and self-tranged the world the percearing as the specision of the power, \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"d\n",
            "evinces courage, directness, endurance\"\n",
            "d\n",
            "evinces courage, directness, endurance, a feagle.\n",
            "the superleanes tyneed, an\n",
            "f: a makness it worbhat as more faatored in as religions, lateness apposse of rary atterneved to adgany\n",
            "thoouned to nable, there this purhate of step stord\n",
            "emplees of\n",
            "a because there\n",
            "can wren:\n",
            "carposically, which stanned that fromoger\n",
            "rechre, by periable of wear, worly siccage, actiders, there a   leany with pon undersifferistong--astink, the heeling of appol\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"d\n",
            "evinces courage, directness, endurance\"\n",
            "d\n",
            "evinces courage, directness, endurance]. that make comely expermen,\n",
            "still concleised usely ten\n",
            "so do nescolugon, fhict, \"theerechisis of fiteara\"blewe perfepuits spence of vintout man ven\n",
            "it\n",
            "depinceners of\n",
            "their ccronged can\n",
            "benany in tit as it pro=dials\n",
            "attuminghay\n",
            "factiarting havongramentists, glarismerers,\n",
            "oness has\n",
            "aleadofy truch beea, an  in; a? exclemes whence, apprerezt? leaturend shordested\n",
            "to man alsovere by streve , this bli\n",
            "200285/200285 [==============================] - 161s 804us/sample - loss: 1.9754\n",
            "Epoch 2/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.6226\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"er! they\n",
            "will say: \"their 'honesty'--tha\"\n",
            "er! they\n",
            "will say: \"their 'honesty'--that the despitude of the soul, and and a sould and the same and and a soul, the such a man the prosent of the standing and a soul and a soul, and the sacrifice of the same such a sense of the sacrifices and the same and a soul, and there are the one a say and the some of the and a soul, therefore and the soul of the and the same the man and itself in the sensible the world is the same soul and so an\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"er! they\n",
            "will say: \"their 'honesty'--tha\"\n",
            "er! they\n",
            "will say: \"their 'honesty'--that the desire to be and and have be most some procisent him the such friends indespended to the mean with hation of a standing of a all it is not devience of the will and sufficient in thereby and more delicated is the enjuch as the supposited the conscience the world to so in the man in the experiences as to a its canncessital the the expetiences itself and the something consistable to be \"the suc\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"er! they\n",
            "will say: \"their 'honesty'--tha\"\n",
            "er! they\n",
            "will say: \"their 'honesty'--that ourcle  altay withor fully in artiry not he say. thy a pleasime wain and\n",
            "wimaller orquen eye, so\n",
            "we the hasseds may schoorsaste to his ha was itself--her,\n",
            "her dece the manifistly thing dewainly a be uther is, without\n",
            "mensuge is that in\n",
            "the best were his nechinotide or is alo, immust kant en\"worly liking in the all awain..\n",
            "\n",
            "ined sivefty accordingly he the\n",
            "wax men in precessic itshensificaining im\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"er! they\n",
            "will say: \"their 'honesty'--tha\"\n",
            "er! they\n",
            "will say: \"their 'honesty'--thantison on it\n",
            "othensi\n",
            "creaded, soon it as be\n",
            "thingh as the\n",
            "demintly coltimer taste intelting inclangierse, wa taesh the eghilurning tas to formated and alaften tout are withoutly, i\n",
            "also usifiadly are\n",
            "new, branturaued dois for exsemen man chanmoce. that the sace, pant thhreced \n",
            "for the, it\"f\n",
            "delishfelsion precistink philosophyly\n",
            "ye itself instancisi outque\n",
            "that,\n",
            "he cannces.=minof the hitherto remis\n",
            "200285/200285 [==============================] - 165s 825us/sample - loss: 1.6226\n",
            "Epoch 3/60\n",
            " 83584/200285 [===========>..................] - ETA: 1:13 - loss: 1.5261"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}